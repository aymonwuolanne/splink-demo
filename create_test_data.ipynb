{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14314969-c13e-4ce4-b8a9-497fde0e7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "# Main idea: to represent family structures, you have a table for families which will reference addresses and surnames, \n",
    "# a table for twins which will reference DOBs, \n",
    "# and then a table for persons which includes sex and fname. \n",
    "\n",
    "def generate_data(n, n_family, n_address):\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    rand_chars = np.array(list(string.ascii_uppercase), dtype=\"str\")\n",
    "\n",
    "    # random strings between 6 and 10 in length\n",
    "    sname_frame = pd.DataFrame({\n",
    "        \"sname_id\": np.arange(n),\n",
    "        \"sname\": np.array([''.join(rng.choice(rand_chars, size=rng.integers(6, 11))) for i in range(n)]),\n",
    "    })\n",
    "\n",
    "    # random strings between 4 and 7 in length\n",
    "    fname_frame = pd.DataFrame({\n",
    "        \"fname_id\": np.arange(n),\n",
    "        \"fname\": np.array([''.join(rng.choice(rand_chars, size=rng.integers(4, 8))) for i in range(n)]),\n",
    "    })\n",
    "    \n",
    "    address_frame = pd.DataFrame({\n",
    "        \"family_id\": rng.integers(low=0, high=n_family, size=n_address),\n",
    "        \"address\": rng.integers(low=0, high=n_family, size=n_address),\n",
    "        \"sa4\": rng.integers(low=0, high=100, size=n_address),\n",
    "    })\n",
    "    \n",
    "    address_frame[\"address\"] = address_frame[\"address\"].astype(\"Int64\")\n",
    "    address_frame[\"sa4\"] = address_frame[\"sa4\"].astype(\"Int64\")\n",
    "    \n",
    "    family_frame = pd.DataFrame({\n",
    "        \"family_id\": np.arange(n_family),\n",
    "        \"sname_id\": rng.geometric(0.01, n_family).astype(int),\n",
    "    })\n",
    "    \n",
    "    twin_frame = pd.DataFrame({\n",
    "        \"twin_id\": np.arange(n),\n",
    "        \"family_id\": rng.integers(low=0, high=n_family, size=n),\n",
    "        \"dob\": pd.to_datetime(rng.integers(low=-10000, high=15000, size=n), unit=\"D\"),\n",
    "    })\n",
    "    \n",
    "    person_frame = pd.DataFrame({\n",
    "        \"person_id\": np.arange(n),\n",
    "        \"twin_id\": np.maximum(np.arange(n) - rng.choice([0, 1], size=n, p=[0.97, 0.03]), 0),\n",
    "        \"fname_id\": rng.geometric(0.02, size=n).astype(int),\n",
    "        \"sex\": rng.choice(['1', '2'], size=n),\n",
    "    })\n",
    "    \n",
    "    person_frame = pd.merge(person_frame, twin_frame, on=[\"twin_id\"])\n",
    "    \n",
    "    person_frame = pd.merge(person_frame, family_frame, on=[\"family_id\"])\n",
    "    \n",
    "    person_frame = pd.merge(person_frame, address_frame, on=[\"family_id\"], how=\"left\")\n",
    "    \n",
    "    person_frame = pd.merge(person_frame, fname_frame, on=[\"fname_id\"], how=\"left\")\n",
    "    \n",
    "    person_frame = pd.merge(person_frame, sname_frame, on=[\"sname_id\"], how=\"left\")\n",
    "    \n",
    "    person_frame = person_frame[[\"person_id\", \"family_id\", \"dob\", \"sex\", \"fname\", \"sname\", \"address\", \"sa4\"]]\n",
    "\n",
    "    return person_frame\n",
    "\n",
    "\n",
    "def create_string_errors(series, rate, position=4):\n",
    "    rng = np.random.default_rng()\n",
    "    result = series.copy()\n",
    "    n = series.shape[0]\n",
    "    rand_chars = np.array(list(string.ascii_uppercase) + [''], dtype=\"str\")\n",
    "    error_flag = rng.choice([False, True], size=n, p=[1-rate, rate])\n",
    "    replacement_chars = rng.choice(rand_chars, size=n)\n",
    "    replacements = result.str.slice(stop=position) + replacement_chars + result.str.slice(start=position+1)\n",
    "    result.loc[error_flag] = replacements.loc[error_flag]\n",
    "    return result\n",
    "\n",
    "\n",
    "def swap_values(series, rate):\n",
    "    rng = np.random.default_rng()\n",
    "    result = series.copy()\n",
    "    n = series.shape[0]\n",
    "    swap_ids = rng.integers(n, size=int(rate * n))\n",
    "    donor_ids = rng.integers(n, size=int(rate * n))\n",
    "    result.iloc[swap_ids] = result.iloc[donor_ids]\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_date_typos(series, rate):\n",
    "    rng = np.random.default_rng()\n",
    "    n = series.shape[0]\n",
    "    result_str = series.dt.strftime(\"%Y%m%d\").copy()\n",
    "    \n",
    "    month_sub = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "\n",
    "    day_sub = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', \n",
    "               '11', '12', '13', '14', '15', '16', '17', '18', '19', '20',\n",
    "               '21', '22', '23', '24', '25', '26', '27', '28']\n",
    "    \n",
    "    # month errors\n",
    "    error_ids = rng.integers(n, size=int(rate / 2 * n))\n",
    "    replacement_chars = rng.choice(month_sub, size=n)\n",
    "    replacements = result_str.str.slice(stop=4) + replacement_chars + result_str.str.slice(start=6)\n",
    "    result_str.iloc[error_ids] = replacements.iloc[error_ids]\n",
    "    # day errors\n",
    "    error_ids = rng.integers(n, size=int(rate / 2 * n))\n",
    "    replacement_chars = rng.choice(day_sub, size=n)\n",
    "    replacements = result_str.str.slice(stop=6) + replacement_chars + result_str.str.slice(start=8)\n",
    "    result_str.iloc[error_ids] = replacements.iloc[error_ids]\n",
    "    result = pd.to_datetime(result_str, format=\"%Y%m%d\", errors=\"coerce\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_missing_values(series, rate):\n",
    "    rng = np.random.default_rng()\n",
    "    n = series.shape[0]\n",
    "    missing_ids = rng.choice(series.index, size=int(n*rate), replace=False)\n",
    "    result = series.copy()\n",
    "    result.iloc[missing_ids] = pd.NA\n",
    "    return result\n",
    "\n",
    "\n",
    "def perturb(df, drop_rate, dup_rate):\n",
    "    # drop some records\n",
    "    fixed_df = df[[\"person_id\", \"family_id\", \"dob\", \"sex\", \"fname\", \"sname\"]].drop_duplicates()\n",
    "\n",
    "    status_df = df[[\"person_id\", \"address\", \"sa4\"]]\n",
    "    \n",
    "    n_fixed = fixed_df.shape[0]\n",
    "\n",
    "    n_status = status_df.shape[0]\n",
    "    \n",
    "    fixed_sample = fixed_df.sample(frac=1-drop_rate)\n",
    "\n",
    "    sample = pd.merge(fixed_sample, status_df, on=[\"person_id\"], how=\"left\")\n",
    "\n",
    "    # create duplicates\n",
    "    dup_sample = pd.concat([\n",
    "        fixed_sample,\n",
    "        fixed_sample.sample(frac=dup_rate),\n",
    "    ]).reset_index(drop=True)\n",
    "\n",
    "    # so that duplicates of the same person_id have a different id\n",
    "    dup_sample[\"unique_id\"] = dup_sample.index\n",
    "\n",
    "    # create errors in the dup_sample columns\n",
    "    dup_sample[\"fname\"] = create_string_errors(dup_sample[\"fname\"], rate=0.1)\n",
    "    dup_sample[\"sname\"] = swap_values(dup_sample[\"sname\"], rate=0.2)\n",
    "    dup_sample[\"sname\"] = create_string_errors(dup_sample[\"sname\"], rate=0.1)\n",
    "    dup_sample[\"dob\"] = create_date_typos(dup_sample[\"dob\"], rate=0.05)\n",
    "\n",
    "    # introduce missingness\n",
    "    dup_sample[\"fname\"] = create_missing_values(dup_sample[\"fname\"], rate=0.03)\n",
    "    dup_sample[\"sname\"] = create_missing_values(dup_sample[\"sname\"], rate=0.01)\n",
    "    dup_sample[\"dob\"] = create_missing_values(dup_sample[\"dob\"], rate=0.01)\n",
    "    dup_sample[\"sex\"] = create_missing_values(dup_sample[\"sex\"], rate=0.03)\n",
    "    \n",
    "    status_dup_sample = pd.merge(dup_sample, status_df, on=[\"person_id\"], how=\"left\")\n",
    "\n",
    "    status_dup_sample[\"address\"] = create_missing_values(status_dup_sample[\"address\"], rate=0.3)\n",
    "\n",
    "    dup_addresses = (\n",
    "        status_dup_sample.loc[~status_dup_sample[\"address\"].isna()]\n",
    "        .groupby(\"unique_id\")[\"address\"]\n",
    "        .agg(list)\n",
    "    )\n",
    "\n",
    "    dup_sa4s = (\n",
    "        status_dup_sample.loc[~status_dup_sample[\"sa4\"].isna()]\n",
    "        .groupby(\"unique_id\")[\"sa4\"]\n",
    "        .agg(list)\n",
    "    )\n",
    "\n",
    "    dup_aggregated = pd.merge(dup_sample, dup_addresses, on=[\"unique_id\"], how=\"left\")\n",
    "    dup_aggregated = pd.merge(dup_aggregated, dup_sa4s, on=[\"unique_id\"], how=\"left\")\n",
    "\n",
    "    return dup_aggregated\n",
    "\n",
    "\n",
    "data = generate_data(n=100_000, n_family=40_000, n_address=80_000)\n",
    "\n",
    "perturbed_data_l = perturb(data, drop_rate=0.0, dup_rate=0.03)\n",
    "perturbed_data_l.to_parquet(\"./perturbed_data_l.parquet\")\n",
    "\n",
    "perturbed_data_r = perturb(data, drop_rate=0.1, dup_rate=0.03)\n",
    "perturbed_data_r.to_parquet(\"./perturbed_data_r.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d37e0-2ed2-47ee-a304-fb6bc23655a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_data_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32315f-63dd-427c-a1c2-77f476583a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splink-env",
   "language": "python",
   "name": "splink-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
